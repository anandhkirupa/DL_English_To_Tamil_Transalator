# English to Tamil Transformer Translator

This is a Streamlit-powered web app that performs English to Tamil translation using a custom **vanilla Transformer** built from scratch with PyTorch.

Built entirely without using Hugging Face, this project demonstrates a clean, from-scratch implementation of the Transformer architecture (encoder-decoder model with attention), tokenization logic, training infrastructure, and deployment flow.

---

## ðŸ§  Features

- Custom Transformer implementation (no prebuilt libraries)
- Sequence-to-sequence translation: English â†’ Tamil
- Custom tokenizer (SimpleTokenizer) with word-level vocab
- Model inference using trained `.pth` weights
- Streamlit frontend for live translation demo
- Deployed to Streamlit Cloud

---

## ðŸš€ Live Demo

[ðŸ‘‰ Click to try the app](https://englishtotamil.streamlit.app/)

---

## ðŸ›  Tech Stack

- Python
- PyTorch (custom Transformer architecture)
- Streamlit (for deployment)
- Pickle (tokenizer persistence)
- Basic file I/O (no database, no nonsense)

---

## ðŸ§ª Model Training

The model was trained using:

- 2-layer encoder and decoder
- `d_model = 128`
- `ff_hidden_dim = 512`
- `n_heads = 8`
- `vocab_size â‰ˆ 185k (src), 464k (tgt)`
- Cross-entropy loss

Due to computational constraints, training was limited to **10 epochs**. The model currently outputs simple translations, and future training (or pretraining on subword-level tokens) can improve accuracy significantly.

---

## Project Structure

english_tamil_transformer/
â”œâ”€â”€ streamlit_app.py # Main Streamlit app
â”œâ”€â”€ transformer_model.py # Full Transformer implementation
â”œâ”€â”€ tokenizer.py # SimpleTokenizer class
â”œâ”€â”€ transformer_en_ta.pth # Trained model weights
â”œâ”€â”€ src_tokenizer.pkl # Tokenizer for English (source)
â”œâ”€â”€ tgt_tokenizer.pkl # Tokenizer for Tamil (target)
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # This file

---

## Why I Built This

This project was developed to:

- Understand and implement the Transformer architecture from scratch
- Explore bilingual translation for low-resource languages
- Practice full ML project lifecycle: data â†’ model â†’ UI â†’ deployment
- Demonstrate core deep learning and deployment skills

---

## Limitations

- Only trained for 10 epochs on limited hardware
- Word-level tokenizer; no subword/BPE
- Limited corpus and vocabulary
- `<unk>` may occur frequently in output (which is a rite of passage)

---
